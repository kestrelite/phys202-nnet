{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: FutureWarning: IPython widgets are experimental and may change in the future.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random, math\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import cm\n",
    "from IPython.html.widgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the code I've developed so far. There's some issue with the backpropagation step, and I have no idea what it is. I'm making some progress, though it's slow and arduous. The test at the bottom is indicative of what the issue is. For a more detailed description of this issue, see [the question I asked on StackOverflow](http://stackoverflow.com/questions/30491307/why-does-this-backpropagation-implementation-).\n",
    "\n",
    "In the meantime, I've littered my code with a smattering of hopefully helpful comments. It's a little messy, and a little bit out of sensible order, but I've made an effort to de-spagghetify it for submission. O, for want of a refactor tool...\n",
    "\n",
    "I should also note up front that much of my work here has been based on [this online text](http://neuralnetworksanddeeplearning.com/chap1.html) on neural networks and backpropagation. The longer I've worked on this, and the more I've referred to the text, the more similar my code has become to the example text online. I apologize for this, and intend to retool this the way I would write it with my understanding once it is functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sigmoid and cost function derivatives\n",
    "def sigmoid_transfer(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "sigmoid_transfer = np.vectorize(sigmoid_transfer)\n",
    "\n",
    "def sigmoid_transfer_deriv(x):\n",
    "    return sigmoid_transfer(x)*(1-sigmoid_transfer(x))\n",
    "sigmoid_transfer_deriv = np.vectorize(sigmoid_transfer_deriv)\n",
    "\n",
    "def linear_transfer(x):\n",
    "    return x;\n",
    "linear_transfer = np.vectorize(linear_transfer)\n",
    "\n",
    "def linear_transfer_deriv(x):\n",
    "    return 1;\n",
    "linear_transfer_deriv = np.vectorize(linear_transfer_deriv)\n",
    "\n",
    "def cost_MSE_deriv(outputs, cost):\n",
    "    return outputs-cost\n",
    "cost_MSE_deriv = np.vectorize(cost_MSE_deriv)\n",
    "\n",
    "transfer = sigmoid_transfer; transfer_deriv = sigmoid_transfer_deriv; cost_deriv = cost_MSE_deriv;\n",
    "def nnet_setup(node_layout, transferp=sigmoid_transfer, \n",
    "               transfer_derivp=sigmoid_transfer_deriv, cost_derivp=cost_MSE_deriv):\n",
    "    global transfer; global transfer_deriv; global cost_deriv;\n",
    "    transfer = transferp; transfer_deriv = transfer_derivp; cost_deriv = cost_derivp;\n",
    "    \n",
    "    weights = []; biases = []\n",
    "    for i in range(1, len(node_layout)):\n",
    "        weights.append(np.random.randn(node_layout[i], node_layout[i-1]))\n",
    "        biases.append(np.random.randn(node_layout[i], 1))\n",
    "    return np.array(weights), np.array(biases)\n",
    "\n",
    "# Basic forward propagation\n",
    "def nnet_prop(weights, biases, inputs):\n",
    "    assert(len(np.array(inputs).shape) == 2)\n",
    "    for w, b in zip(weights, biases):\n",
    "        inputs = transfer(np.dot(w, inputs) + b)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following functions split data into randomized subsets\n",
    "\n",
    "# Splits the data set into a randomized training and test set\n",
    "# with format [([image_matrix], exp_value), ([image_matrix], exp_value)...]\n",
    "def split_set(dataset, point):\n",
    "    training_temp = list(zip(dataset.images[:point], dataset.target[:point]))\n",
    "    test_temp = list(zip(dataset.images[point:], dataset.target[point:]))\n",
    "    random.shuffle(training_temp)\n",
    "    random.shuffle(test_temp)\n",
    "    training_set, training_sols = zip(*training_temp)\n",
    "    test_set, test_sols = zip(*test_temp)\n",
    "    return list(zip(training_set, training_sols)), list(zip(test_set, test_sols))\n",
    "\n",
    "# Splits training data into batches with format\n",
    "# [[([image_mat], exp_value), ([image_mat], exp_value), ...], ...]\n",
    "def split_to_batch(trainset, size):\n",
    "    return [trainset[n*size:(n+1)*size] for n in range(0, math.floor(len(trainset)/size))]\n",
    "\n",
    "# Converts an image matrix into a column vector\n",
    "def conv_to_col(vec, scale=1):\n",
    "    vec = np.array(vec)\n",
    "    assert(len(vec.shape) == 2)\n",
    "    return np.rot90([vec.reshape((vec.shape[0] * vec.shape[1]))])/scale\n",
    "\n",
    "# Rotates a list into a numpy column vector\n",
    "def rotate_list(vec):\n",
    "    return np.rot90([np.array(vec).reshape((len(vec)))])\n",
    "\n",
    "# Creates a target training vector \n",
    "# [0, 0, 0, 0, 0, 0....] + e_pos (e being basis vector)\n",
    "def create_tgt_vec(pos, length=10):\n",
    "    tmp = np.zeros(length)\n",
    "    tmp[pos] = 1\n",
    "    return np.rot90([tmp])\n",
    "\n",
    "def read_outp_vec(vec):\n",
    "    maxInd = 0;\n",
    "    for i in range(1, len(vec)+1):\n",
    "        if vec[-i][0] > vec[-maxInd][0]: maxInd = i\n",
    "    if maxInd == 0: maxInd = len(vec)\n",
    "    return maxInd - 1\n",
    "\n",
    "digits = load_digits()\n",
    "# For debugging purposes, this loads the training set\n",
    "# with one batch and one test image in that batch\n",
    "train_set, test_set = split_set(digits, 400)\n",
    "train_set = split_to_batch(train_set, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backpropagate(inp, outp, wts, bias, outp_length=10):\n",
    "    del_w = [np.zeros(shape=wt.shape) for wt in wts]\n",
    "    del_b = [np.zeros(shape=bt.shape) for bt in bias]\n",
    "    \n",
    "    next_input = conv_to_col(inp)\n",
    "    outp = create_tgt_vec(outp, length=outp_length)\n",
    "    \n",
    "    pre_trans = []; post_trans = []\n",
    "    for w, b in zip(wts, bias):\n",
    "        next_input = np.dot(w, next_input) + b\n",
    "        pre_trans.append(next_input)\n",
    "        next_input = transfer(next_input)\n",
    "        post_trans.append(next_input)\n",
    "    \n",
    "    #print(\"O: \", outp)\n",
    "    delta = cost_deriv(post_trans[-1], outp) * transfer_deriv(pre_trans[-1])\n",
    "    del_b[-1] = delta\n",
    "    del_w[-1] = np.dot(delta, post_trans[-2].transpose())\n",
    "    \n",
    "    for i in range(2, len(wts)):\n",
    "        pre_tr_vec = pre_trans[-i]\n",
    "        tr_deriv = transfer_deriv(pre_tr_vec)\n",
    "        delta = np.dot(wts[-i+1].transpose(), delta) * tr_deriv\n",
    "        del_b[-i] = delta\n",
    "        del_w[-i] = np.dot(delta, post_trans[-i-1].transpose())\n",
    "    \n",
    "    return del_w, del_b\n",
    "\n",
    "def SGD(train_set, wts, bias, eta, backprop_fn=backpropagate, outp_length=10, decay_rate=0.0):\n",
    "    training_size = 0\n",
    "    if len(train_set) == 0: training_size = 1\n",
    "    else: training_size = len(train_set[0])\n",
    "\n",
    "    learning_coef = eta / training_size\n",
    "    \n",
    "    for next_set in train_set:\n",
    "        sum_del_w = [np.zeros(w.shape) for w in wts]\n",
    "        sum_del_b = [np.zeros(b.shape) for b in bias]\n",
    "        \n",
    "        for inp, outp in next_set:\n",
    "            next_del_w, next_del_b = backprop_fn(inp, outp, wts, bias, outp_length=outp_length);\n",
    "            sum_del_w = [nw + dw for nw, dw in zip(next_del_w, sum_del_w)];\n",
    "            sum_del_b = [nb + db for nb, db in zip(next_del_b, sum_del_b)];\n",
    "        \n",
    "        wts  = [wt - learning_coef * (dw + decay_rate * wt) for wt, dw in zip(wts, sum_del_w)]\n",
    "        bias = [bt - learning_coef * (db + decay_rate * bt) for bt, db in zip(bias, sum_del_b)]\n",
    "    return wts, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New max: 0.2290622763063708 (0)\n",
      "New max: 0.2970651395848246 (1)\n",
      "New max: 0.4309234073013601 (2)\n",
      "New max: 0.4423765211166786 (3)\n",
      "New max: 0.46313528990694347 (4)\n",
      "New max: 0.47959914101646384 (5)\n",
      "New max: 0.5139584824624195 (6)\n",
      "New max: 0.5375805297065139 (7)\n",
      "New max: 0.5676449534717252 (9)\n",
      "New max: 0.5962777380100215 (15)\n",
      "New max: 0.6020042949176807 (17)\n",
      "New max: 0.6070150322118826 (19)\n",
      "New max: 0.6163206871868289 (29)\n",
      "New max: 0.6206156048675734 (30)\n",
      "New max: 0.6220472440944882 (33)\n",
      "New max: 0.6249105225483178 (38)\n",
      "New max: 0.6306370794559771 (39)\n",
      "New max: 0.6471009305654974 (40)\n",
      "New max: 0.6499642090193272 (43)\n",
      "New max: 0.6649964209019327 (48)\n",
      "Second run\n",
      "New max: 0.2949176807444524 (0)\n",
      "New max: 0.4617036506800286 (1)\n",
      "New max: 0.5518969219756621 (2)\n",
      "New max: 0.6048675733715104 (3)\n",
      "New max: 0.6091624910522548 (5)\n",
      "New max: 0.6478167501789549 (7)\n",
      "New max: 0.6599856836077308 (8)\n",
      "New max: 0.6757337151037939 (14)\n",
      "New max: 0.677881173944166 (19)\n",
      "New max: 0.6893342877594846 (21)\n",
      "New max: 0.690050107372942 (23)\n",
      "New max: 0.7043664996420902 (30)\n",
      "New max: 0.7151037938439513 (40)\n",
      "New max: 0.7201145311381532 (43)\n",
      "New max: 0.7365783822476736 (47)\n",
      "Average run\n",
      "(609, 1397, 0.4359341445955619)\n",
      "0.7301360057265569\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(wts, bias, test_set):\n",
    "    correct = 0; total = 0;\n",
    "    for i in test_set:\n",
    "        out = read_outp_vec(nnet_prop(wts, bias, conv_to_col(i[0])))\n",
    "        total = total + 1\n",
    "        if out == i[1]: correct = correct + 1\n",
    "    return correct, total, float(correct)/float(total)\n",
    "\n",
    "def average_network(wts1, bias1, wts2, bias2):\n",
    "    return 0.5 * (wts1 + wts2), 0.5 * (bias1 + bias2)\n",
    "\n",
    "\n",
    "\n",
    "s = 0.0; t = 1.0\n",
    "networks = []\n",
    "for i in range(0, int(t)):\n",
    "    train_set, test_set = split_set(digits, 400)\n",
    "    wts, bias = nnet_setup([64, 30, 20, 10])#, transferp=linear_transfer, transfer_derivp=linear_transfer_deriv)\n",
    "    #print(evaluate(wts, bias, test_set))\n",
    "    wts_maxA = wts; bias_maxA = bias; effectivenessA = evaluate(wts, bias, test_set)[2]\n",
    "    for j in range(0, 50):\n",
    "        random.shuffle(train_set)\n",
    "        train_set_split = split_to_batch(train_set, 2)\n",
    "        wts, bias = SGD(train_set_split, wts, bias, 1.3, decay_rate=0.0003)\n",
    "        #outputs.append(evaluate(wts, bias, test_set)[2])\n",
    "        #print(\"Step \" + str(j+1) + \": \" + str(outputs[-1]))\n",
    "        new_effect = evaluate(wts, bias, test_set)[2]\n",
    "        if new_effect > effectivenessA:\n",
    "            effectivenessA = new_effect\n",
    "            wts_maxA = wts; bias_maxA = bias\n",
    "            print(\"New max: \" + str(new_effect) + \" (\" + str(j) + \")\")\n",
    "            \n",
    "    networks.append((wts_maxA, bias_maxA))\n",
    "\n",
    "    print(\"Second run\")\n",
    "    train_set, test_set = split_set(digits, 400)\n",
    "    wts, bias = nnet_setup([64, 30, 20, 10])#, transferp=linear_transfer, transfer_derivp=linear_transfer_deriv)\n",
    "    #print(evaluate(wts, bias, test_set))\n",
    "    wts_maxB = wts; bias_maxB = bias; effectivenessB = evaluate(wts, bias, test_set)[2]\n",
    "    for j in range(0, 50):\n",
    "        random.shuffle(train_set)\n",
    "        train_set_split = split_to_batch(train_set, 2)\n",
    "        wts, bias = SGD(train_set_split, wts, bias, 1.3, decay_rate=0.0003)\n",
    "        #outputs.append(evaluate(wts, bias, test_set)[2])\n",
    "        #print(\"Step \" + str(j+1) + \": \" + str(outputs[-1]))\n",
    "        new_effect = evaluate(wts, bias, test_set)[2]\n",
    "        if new_effect > effectivenessB:\n",
    "            effectivenessB = new_effect\n",
    "            wts_maxB = wts; bias_maxB = bias\n",
    "            print(\"New max: \" + str(new_effect) + \" (\" + str(j) + \")\")\n",
    "    \n",
    "    print(\"Average run\")\n",
    "    wts, bias = average_network(np.array(wts_maxA), np.array(bias_maxA), \n",
    "                                      np.array(wts_maxB), np.array(bias_maxB))\n",
    "    train_set, test_set = split_set(digits, 400)\n",
    "    print(evaluate(wts, bias, test_set))\n",
    "    for j in range(0, 50):\n",
    "        random.shuffle(train_set)\n",
    "        train_set_split = split_to_batch(train_set, 2)\n",
    "        wts, bias = SGD(train_set_split, wts, bias, 1.3, decay_rate=0.0003)\n",
    "        \n",
    "    print(evaluate(wts, bias, test_set)[2])\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unit Tests\n",
    "\n",
    "# Sigmoid transfer\n",
    "assert(np.allclose(sigmoid_transfer([-0.5, 0, 1]), [0.3775, 0.5, 0.7311], 0.01))\n",
    "assert(sigmoid_transfer(-100) >= 0 and sigmoid_transfer(100) <= 1)\n",
    "assert(np.allclose(sigmoid_transfer_deriv([-0.5, 0, 1]), [0.235, 0.25, 0.1966], 0.001))\n",
    "\n",
    "# Linear transfer\n",
    "assert(np.allclose(linear_transfer([1, 100, -1000]), [1, 100, -1000], 0))\n",
    "assert(np.allclose(linear_transfer_deriv([1, 100, -1000]), [1, 1, 1], 0))\n",
    "\n",
    "# MSE cost function\n",
    "assert(np.allclose(cost_MSE_deriv([1, 0], [0, -1]), [1, 1], 0))\n",
    "\n",
    "# NNet setup shaping\n",
    "assert(nnet_setup([1, 2, 1])[0][0].shape == (2, 1))\n",
    "assert(nnet_setup([1, 2, 1])[0][1].shape == (1, 2))\n",
    "assert(nnet_setup([1, 2, 1])[1][0].shape == (2, 1))\n",
    "assert(nnet_setup([1, 2, 1])[1][1].shape == (1, 1))\n",
    "\n",
    "# Basic feedforward testing\n",
    "wts, bias = nnet_setup([1, 2, 1], linear_transfer, linear_transfer_deriv)\n",
    "for i in wts: \n",
    "    for j in i: j.fill(1)\n",
    "for i in bias: \n",
    "    for j in i: j.fill(0)\n",
    "out = nnet_prop(wts, bias, [[1]])\n",
    "assert(len(out.shape) == 2)\n",
    "assert(out[0][0] == 2)\n",
    "\n",
    "for i in wts:\n",
    "    for j in i: j.fill(1)\n",
    "for i in bias:\n",
    "    for j in i: j.fill(-1)\n",
    "\n",
    "assert(nnet_prop(wts, bias, [[1]]) == -1)\n",
    "\n",
    "# Batch creation testing\n",
    "digits = load_digits()\n",
    "train_set1, test_set1 = split_set(digits, 10)\n",
    "train_set2, test_set2 = split_set(digits, 10)\n",
    "\n",
    "assert(len(train_set1) == 10)\n",
    "assert(len(test_set1) == len(digits.images) - len(train_set1))\n",
    "assert(type(test_set1[0]) == tuple)\n",
    "assert(not(np.allclose(test_set1[0][0], test_set2[0][0], 0)))\n",
    "\n",
    "batch_1 = split_to_batch(train_set1, 2)\n",
    "batch_2 = split_to_batch(train_set2, 2)\n",
    "assert(len(batch_1) == 5)\n",
    "assert(len(batch_1[0]) == 2)\n",
    "assert(len(batch_1[0][0]) == 2)\n",
    "assert(batch_1[0][0][0].shape == (8, 8))\n",
    "\n",
    "image_1 = batch_1[0][0][0]\n",
    "assert(np.allclose(conv_to_col([[1, 2], [3, 4]]), [[4],[3],[2],[1]], 0))\n",
    "assert(conv_to_col(image_1).shape == (64, 1))\n",
    "\n",
    "# Output reads correctly\n",
    "assert(read_outp_vec(create_tgt_vec(5)) == 5)\n",
    "assert(read_outp_vec(create_tgt_vec(0)) == 0)\n",
    "assert(read_outp_vec(create_tgt_vec(9)) == 9)\n",
    "\n",
    "# SGD testing\n",
    "def mock_backprop(inp, outp, wts, bias, outp_length=10):\n",
    "    return wts, bias\n",
    "\n",
    "wts, bias = nnet_setup([1, 2, 1])\n",
    "for i in wts:\n",
    "    for j in i:\n",
    "        j.fill(1)\n",
    "for i in bias:\n",
    "    i.fill(1)\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    wts, bias = SGD([[(1, 1)]], wts, bias, 0.1, backprop_fn=mock_backprop)\n",
    "for i in wts:\n",
    "    for j in i:\n",
    "        assert(np.allclose(j, 0, 0.001))\n",
    "for i in bias:\n",
    "    assert(np.allclose(i, 0, 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
