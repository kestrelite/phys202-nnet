{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Neural Networks\n",
    "\n",
    "Neural networks are a method for automatic decision-making and classification, more commonly known as weak AI. The idea developed out of synaptic connections in the brain, which output signals based on the magnitude of the incoming signals. The stronger the incoming signals, the greater the chance the neuron will activate.\n",
    "\n",
    "In neural networks, neurons are represented by nodes, which are grouped into layers. Simple neural networks are split into three parts:\n",
    "\n",
    " - The **input layer**, which accepts inputs from the \"outside world\" - for instance, an image of a digit. \n",
    " - The **hidden layer**, which can really be thought of as a processing layer, where computations happen.\n",
    " - The **output layer**, which returns processed information.\n",
    " \n",
    "![Image courtesy the OpenCV project](http://docs.opencv.org/_images/mlp.png)\n",
    "\n",
    "In this image, courtesy the OpenCV project, there are three input nodes, five hidden layer nodes, and two output nodes. Each node in a successive layer receives values from all of the nodes in the layer before it. Let's create this network now, as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nnet_core import *\n",
    "from transfer_funcs import linear_transfer, linear_transfer_deriv\n",
    "from matplotlib.pylab import cm\n",
    "from moviepy.video.io.bindings import mplfig_to_npimage\n",
    "import moviepy.editor as mpy\n",
    "\n",
    "# This will start with randomized weights and biases\n",
    "weights, biases = nnet_setup([3, 5, 2], transferp=linear_transfer, transfer_derivp=linear_transfer_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A synapse in the brain can be emulated as a device with a _threshold_ and an _output_. If the inputs to the synapse are greater than the threshold, then the output is triggered. However, unlike the synapses in the brain, which must either output a 1 or 0, neural networks allow for dyanmic input and dyanmic output. To do this, nodes have two values associated with them: a weight $w$ and a bias $b$.\n",
    "\n",
    "In order for a node to transmit a positive value, its input has to exceed its bias. This gives us the formula $x*w-b$ for a single connection. We can then write all incoming connections to a single node as follows:\n",
    "\n",
    "Let $w_{ij}$ be the weight from node $i$ to the node $j$ between two layers, and let $b_j$ be the bias of the node $j$ in the second layer. (Note that biases are specific to the _node_, not the _connection_.) Then, the total information that goes to a single node from the previous layer is: $$\\sum_i x_i * w_{ij} - b_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural networks as matrix multiplication**\n",
    "\n",
    "You might notice something in the previous notation: $x_i$ and $b_j$ form column vectors, and $w_{ij}$ can form a matrix. This does hold true; consequently, we can write the computation between layers as such:\n",
    "\n",
    "$$\\left(\\begin{matrix}w_{11} & \\cdots & w_{1i} \\\\ \\vdots & & \\vdots \\\\ w_{j1} & \\cdots & w_{ij} \\end{matrix}\\right) \\left(\\begin{matrix}x_1 \\\\ \\vdots \\\\ x_i \\end{matrix}\\right) - \\left(\\begin{matrix}b_1 \\\\ \\vdots \\\\ b_i \\end{matrix}\\right)$$\n",
    "\n",
    "This makes it very easy to compute, and that is how the following function is implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"From 3 nodes to 5 nodes, the weight matrix has the shape \" + str(weights[0].shape))\n",
    "\n",
    "nnet_prop(weights, biases, np.rot90([[1, 1, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Giving it something to learn**\n",
    "\n",
    "As a demonstration, let's say we wanted to teach it to output higher on the first node if the first two numbers are higher than the third, and output higher on the second node if they aren't. Let's test this over the interval $[1, 10]$ for the training data.\n",
    "\n",
    "$$f(x, y, z) = \\left\\{\\begin{array}{ll} 1 & x \\geq z \\;\\mbox{and}\\; y \\geq z \\\\ 0 & \\mbox{else}\\end{array}\\right.$$ \n",
    "\n",
    "First, we need to generate some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset_mgmt import *\n",
    "import random as rand\n",
    "\n",
    "training_data_count = 200\n",
    "\n",
    "training_data = []\n",
    "test_data = []\n",
    "for i in range(0, training_data_count * 2):\n",
    "    inp = [rand.randint(1, 10), rand.randint(1, 10), rand.randint(1, 10)]    \n",
    "    if(inp[2] == sorted(inp)[::-1][2]): \n",
    "        training_data.append(([inp], 1))\n",
    "    else: \n",
    "        training_data.append(([inp], 0))\n",
    "\n",
    "test_data = training_data[training_data_count:]\n",
    "training_data = split_to_batch(training_data[:training_data_count], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did I use 0 and 1 instead of column vectors? Part of that is a limitation of the way I wrote this library, as I wrote it with digit recognition in mind. They don't have to be, however, the utility functions I have do a handy job of turning those into target vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(create_tgt_vec(0, length=2), \"\\n\")\n",
    "print(create_tgt_vec(1, length=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic gradient descent and backpropagation**\n",
    "\n",
    "Now it's time to see how the network actually learns something. First, it's helpful to see what it does. The code below makes a copy of the neural network we created earlier, then trains it 50 times (\"epochs\") against the training data we created. The data is split into subgroups of 10 items, since this helps it classify various inputs at the same time, so with 200 items in the test set, there will be 20 data sets.\n",
    "\n",
    "_Note:_ Some networks will train better than others. If you get an uninteresting graph, go up to the top and create a new random network to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a copy for playing with\n",
    "if False: weights, biases = nnet_setup([3, 5, 2], transferp=linear_transfer, transfer_derivp=linear_transfer_deriv)\n",
    "wts, bias = weights, biases\n",
    "print(\"Starting accuracy: \" + str(100 * nnet_evaluate_single(wts, bias, test_data)[2]) + \"%\")\n",
    "\n",
    "iteration = []\n",
    "evaluation = []\n",
    "\n",
    "for i in range(0, 25):\n",
    "    iteration.append(i+1)\n",
    "    evaluation.append(nnet_evaluate_single(wts, bias, test_data)[2] * 100)\n",
    "    wts, bias = nnet_SGD(training_data, wts, bias, 0.0008, outp_length=2)\n",
    "    \n",
    "print(\"Ending accuracy: \" + str(100 * nnet_evaluate_single(wts, bias, test_data)[2]) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(iteration, evaluation)\n",
    "plt.ylim([min(evaluation), 100]); plt.xlim([1, max(iteration)])\n",
    "plt.xlabel(\"Training epoch number\")\n",
    "plt.ylabel(\"Percent accuracy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what's happening, recall that the gradient of a vector always points in the direction of greatest decrease. Neural networks (ab)use this property to minimize the error of the outputs of the network. This is, in truth, the only fundamental principle at work here. We can adjust the parameters of the network to move in the direction that minimizes error. But how fast do we move? \n",
    "\n",
    "To answer that, the network uses a rate of movement, heretofore called the _learning rate_, represented by $\\eta$. Let's call the error a _cost function_ with respect to an input vector $\\vec{x}$. Then, for each $w_{ij}$, we can compute the derivative of the cost: $$\\frac{dC(\\vec{x})}{dw_{ij}}$$\n",
    "\n",
    "This represents how quickly the error changes with respect to the weight. All in all, we're setting up a differential equation. $$w_{ij}' = w_{ij} - \\eta \\frac{dC(\\vec{x})}{dw_{ij}}$$\n",
    "\n",
    "Then, we can do the same thing for biases. What's the end result? A movement like this:\n",
    "\n",
    "![](http://blog.datumbox.com/wp-content/uploads/2013/10/gradient-descent.png)\n",
    "\n",
    "(Courtesy datumbox)\n",
    "\n",
    "However, notice how this network seems to reach a maximum effectiveness? Also notice how the gradients have different minima?\n",
    "\n",
    "This is one of the problems with neural networks. There may be an absolute minimum of the network, but you won't be able to get there with stochastic gradient descent. Instead, we need a different tools to get us to higher accuracy. There are a number of these, which are better saved for a different overview.\n",
    "\n",
    "**Transfer functions**\n",
    "\n",
    "An overview would be incomplete without a brief discussion of transfer functions. A transfer function is a modifier that is applied to the output from each node in the network. I have been implicitly assuming that all transfer functions are linear; that is, that $$T(\\vec{x}) = \\vec{x}$$\n",
    "\n",
    "This does not, however, have to be the case. While it is most effective for the above training regime, for the actual classification problem, a sigmoidal function is advantageous. The sigmoid follows: $$\\sigma(\\vec{x}) = \\frac{1}{1-e^{\\vec{x}}}$$\n",
    "\n",
    "This is particularly advantageous because it only outputs between $(0, 1)$, and because the shape of the curve leads to small changes at extrema. A plot of the sigmoid is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transfer_funcs import sigmoid_transfer\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = sigmoid_transfer(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlim(-5, 5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting these things together, we now have the following function for a change between layers:\n",
    "\n",
    "$$\\sigma\\left[\\left(\\begin{matrix}w_{11} & \\cdots & w_{1i} \\\\ \\vdots & & \\vdots \\\\ w_{j1} & \\cdots & w_{ij} \\end{matrix}\\right) \\left(\\begin{matrix}x_1 \\\\ \\vdots \\\\ x_i \\end{matrix}\\right) - \\left(\\begin{matrix}b_1 \\\\ \\vdots \\\\ b_i \\end{matrix}\\right)\\right]$$\n",
    "\n",
    "However, there's one more optional piece to tack on: **Weight decay.**\n",
    "\n",
    "Weight decay is used to deteriorate the weights on a network at a very slow rate. This prevents the network from generating noise, as the original random distribution of weights is wont to do, by reducing irrelevant weights over time. It can also result in a higher network success rate, as it will stop the network from falling into tiny local minima. \n",
    "\n",
    "In practice, a weight decay is very simple to implement. It's a scalar of the current weight value, so if $\\gamma$ is the decay rate, the term becomes: $$-\\eta\\,\\gamma\\, w_{ij}$$\n",
    "\n",
    "All said, the final training method used in this network can be expressed as follows:\n",
    "\n",
    "$$w'_{ij} = w_{ij} - \\eta\\frac{dC(\\vec{x})}{dw_{ij}} - \\eta\\,\\gamma\\, w_{ij}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
