{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Search for Better Initial Conditions\n",
    "\n",
    "In modeling, there is a method known as the _Monte Carlo Hypercube_. For effective use of a hypercube, one needs to have a search space that is too large to iterate through, and potentially too many outputs to monitor individually. A hypercube allows you to draw correlations between large amounts of parameters and outputs using far fewer iterations and tests than would otherwise be necessary.\n",
    "\n",
    "It accomplishes this feat through the magic of random numbers. By selecting random items over the search space, it becomes possible to create enough data to interpolate relevant results representative of the average without strong testing. An implicit assumption of the hypercube is that the search space is continuous; if it is discontinuous, errors and nonlinearities will pop up in the output data. \n",
    "\n",
    "However, these neural networks are, thankfully, a fully linear and continuous system. (If it were not, the gradient of the cost would be discontinuous. There are systems for which that is the case, but this is not one of them.)\n",
    "\n",
    "What follows is a brief discussion on the oddities that result from hypercube searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's just get these out of the way...\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import cm\n",
    "from IPython.html.widgets import interact\n",
    "import pickle\n",
    "\n",
    "from dataset_mgmt import *\n",
    "from nnet_core import *\n",
    "from transfer_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data has been pregenerated (it takes a few hours - reduced from a week)\n",
    "dataset = pickle.load(open('single_layer_evaluation.pickle', 'rb'))\n",
    "\n",
    "data_names = ('Net Quantity', 'Net Keep Qty', 'Learning Rate', 'Weight Decay Rate',  \n",
    "         'First Layer Nodes', 'Second Layer Nodes', 'Learning Epochs', \n",
    "         'Training Set Size', 'Training Batch Size', 'Effectiveness')\n",
    "\n",
    "for name in data_names:\n",
    "    dataset[name] = [x for y,x in sorted(zip(dataset['Effectiveness'], dataset[name]))[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following interactive plot shows how all the tested parameters compare against the network effectiveness. There are a few notable results that come out of this plot.\n",
    "\n",
    "The most significant result is that the neural network tuning paramets must all be inter-related. No matter which plot is selected, it is _possible_ to create high success rates no matter the value. An averaging plot later will show more information on this. \n",
    "\n",
    "Two parameters were additionally tested: a number of networks created in a random forest, and a number of networks selected out of this pool. More information on this is available in [NNet Averaging](NNet_Averaging.ipynb). However, the plot of the network keep quantity ($m$ in the averaging notebook) versus effectiveness seems to corroborate the claim the other notebook notes that the more networks are kept, the greater chance the network will have a higher accuracy. \n",
    "\n",
    "The plot of learning rate versus effectiveness seems to be scattered evenly - there are high accuracy networks with low learning rates, and low accuracy networks with high learning rates. This suggests that the learning rate is less relevant than the number of epochs and/or other parameters, but further topological classification methods would be necessary to discern which parameters are relevant to this plot. \n",
    "\n",
    "Similarly, the number of nodes also seems less relevant - to a point. The highest value seems to drop the closer to zero the network goes, which is predictable. Past that, the plot is unclear - further data analysis as presented below shows better results. The same result shows for learning epochs and training set size, though training set size has a clearer relationship. By way of contrast, batch size seems to have a negative relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_plot(dataname):\n",
    "    plt.scatter(dataset[dataname], dataset['Effectiveness'], alpha=0.30)\n",
    "    plt.gcf().set_size_inches(8, 6)\n",
    "    plt.xlabel(dataname)\n",
    "    plt.ylabel('Effectiveness')\n",
    "    if dataname == 'Weight Decay Rate': plt.xlim([-0.0000, 0.0011])\n",
    "    if dataname == 'First Layer Nodes': plt.xlim([5, 30])\n",
    "    if dataname == 'Training Set Size': plt.xlim([0, 1000])\n",
    "    plt.title(dataname + ' vs. Effectiveness')\n",
    "    \n",
    "interact(create_plot, dataname=data_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By converting the data to an averaged plot, it becomes far easier to see the relationship between the various parameters and network effectiveness. This tells us something important: that while it is _possible_ to generate good networks with small parameters, it is more _consistently doable_ with higher ones. \n",
    "\n",
    "The rest of this data can speak for itself.\n",
    "\n",
    "**Note:** With high smoothing, some boundary errors may be present as a consequence of convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_avg_plot(dataname, smoothing=2):\n",
    "    data = dataset[dataname]\n",
    "    eff = dataset['Effectiveness']\n",
    "    boxes = []\n",
    "    if type(min(data)) == float: \n",
    "        if dataname == 'Learning Rate': boxes = np.linspace(5, 15, 11) * 0.1\n",
    "        if dataname == 'Weight Decay Rate': boxes = np.linspace(0, 0.0010, 11)\n",
    "    else: boxes = np.linspace(min(data), max(data), max(data) - min(data) + 1)\n",
    "    \n",
    "    new_boxes = []\n",
    "    new_eff = []\n",
    "    for i in boxes:\n",
    "        tmp = [y for x, y in zip(data, eff) if x == i]\n",
    "        if len(tmp) == 0: continue\n",
    "        new_eff.append(sum(tmp) / float(len(tmp)))\n",
    "        new_boxes.append(i)\n",
    "    if len(new_eff) == 0: return\n",
    "    \n",
    "    new_eff_smooth = np.convolve(new_eff, np.ones(smoothing)/float(smoothing), 'same')\n",
    "\n",
    "    plt.plot(new_boxes, new_eff_smooth)\n",
    "    plt.gcf().set_size_inches(8, 6)\n",
    "    plt.xlabel(dataname)\n",
    "    plt.ylabel('Effectiveness')\n",
    "    plt.title('Average ' + dataname + ' vs. Effectiveness')\n",
    "    \n",
    "interact(create_avg_plot, dataname=data_names, smoothing=(1, 10, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two demonstrations take the highest scoring $n$ networks out of the data analysis pool and shows their average parameters. \n",
    "\n",
    "This lets us see where the optimal points are for network training, and more easily watch what increases and decreases as network moves. In the second plot, an approximately flat line implies that the function has little to no impact - its value remains around its average regardless of the selectivity. An increasing or a decreasing plot implies a correlation between success rate and that parameter.\n",
    "\n",
    "In the last plot, the graph of effectiveness versus the top networks in the pool actually shows an interesting result: that if we pick random parameters on the line, we're very likely to get a final effectiveness along a linear distribution. Interestingly, most of the relationships are linear, which implies that we should prefer parameters which increase with $O(1)$ or $O(n)$ computational time complexity over those which increase with $O(n^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_setup(count):\n",
    "    for name in data_names:\n",
    "        print(name + \": \", float(sum(dataset[name][0:count]))/float(count))\n",
    "        \n",
    "interact(best_setup, count=(1, len(dataset['Effectiveness'])));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_setup_plot(dataname):\n",
    "    y_data = np.array(dataset[dataname])\n",
    "    y_axis = []\n",
    "    for i in range(0, len(y_data)):\n",
    "        var = sum(y_data[0:i+1])/float(i+1)\n",
    "        y_axis.append(var)\n",
    "    plt.plot(range(1, len(y_data)+1), y_axis)\n",
    "    plt.xlabel(\"Top $n$ networks in data set\")\n",
    "    plt.ylabel(dataname)\n",
    "    plt.title(dataname + \" per top $n$ networks in data set\")\n",
    "    plt.gcf().set_size_inches(8, 6)\n",
    "\n",
    "interact(best_setup_plot, dataname=data_names);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
